\documentclass{article}

\title{Mathematical Techniques for Computer Science - Notes}
\author{Harvey Hyatt}
\date{}

\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}

\begin{document}
\maketitle

\section{Gaussian Elimination}
Gaussian Elimination is a recursive algorithm - it has a base case and a general case:
\begin{description}
\item[Base Case]
There is only one equation and one unknown: $ax=b$. This can be solved as $x=\frac{b}{a}$.
\item [General Case]
There are $n$ equations and each contains $n$ unknowns. We use the first equation to eliminate the first unknown in the other $n-1$ equations. Apply the algorithm recursively to these $n-1$ equations.
\end{description}
This should lead to a staircase pattern of zeros.
$$\begin{pmatrix}
\bullet & * & * & * \\
0 & \bullet & * & * \\
0 & 0 & \bullet & * \\
0 & 0 & 0 & \bullet
\end{pmatrix}$$
The term for this is \textbf{echelon form}.

\section{Gaussian Elimination: Special Cases}
These are all to do with zeros appearing in the ``wrong place''.
\paragraph{Contradictory equation}
If, in the base case $ax=b$, $a=0$ and $b\neq 0$, then the equation is contradictory since $0=b$. In this case there is \textit{no solution}.

\paragraph{Irrelevant equation}
This is like the last case but this time both sides are zero, i.e. the equation reduces to $0=0$. This means that the value of x can be \textbf{chosen freely}.

\paragraph{Can't use the first equation to eliminate the first unknown}
If the coefficient of the first unknown in the first equation is zero, then the first variable of the other equations cannot be eliminated.
\vspace{1mm}\\
The solution is to exchange the first equation with another equation where the first coefficient is not zero, and run the algorithm on this rearranged matrix. If the calculation is being done by a human, we would like the coefficient to be as close to 1 as possible.

\paragraph{Can't use any of the equations to eliminate the first unknown}
Similar to the previous case, this means that the first variable is not constrained at all and can be chosen freely. However, this creates a system where we have \textit{two equations} for \textit{one unknown}. We have to consider more general systems of linear equations.

\paragraph{General Gaussian elimination}
The General Case stays the same - we only need to reconsider the base case.
\begin{description}
\item [Base Case 1] There are multiple equations but only one unknown left. Solve each equation independantly and compare the answers: if all the same, then that is the solution. If they disagree, then the system has no solution.
\item [Base Case 2] There are multiple (say $m$ many) unknowns but only one equation left. In this case $m-1$ unknowns can be chosen freely and the other one is computed from the equation.
\end{description}

\paragraph{Special cases in Echelon form}
Suppose after performing Gaussian elimination we end up with this matrix:
$$\begin{pmatrix}
  0 & \bullet & * & * & * & * \\
  0 & 0 & 0 & \bullet & * & * \\
  0 & 0 & 0 & 0 & \bullet & * \\
  0 & 0 & 0 & 0 & 0 & ? \\
  0 & 0 & 0 & 0 & 0 & 0
\end{pmatrix}$$
Here the bullets represent non-zero numbers and the asterisks represent arbitrary numbers. If the question mark is non-zero, there is no solution (since we have contradictory equations). If it is zero, then we have infinitely many solutions, since some variables can be chosen freely. The ones that can be chosen freely are the ones where the staircase does not drop down one level (in this case $x_1$ and $x_3$). The value of certain other variables will depend on what is chosen for $x_1$ and $x_3$.

\paragraph{Fields}
There are many \textit{number systems} (also known as \textbf{fields}) that can take the place of rational (or real) numbers. The requirements of a field are:
\begin{itemize}
\item Elements of the field can be added and multiplied.
\item There is a ``zero'' and a ``one'', which we write as 0 and 1.
\end{itemize}
Therefore the following two equations must be satisfied:
\begin{gather*}
x+0=x\\
x*1=x
\end{gather*}
\begin{align*}
x+y&=y+x & x*y&=y*x & x*(y+z)&=x*y+x*z\\
x+(y+z)&=(x+y)+z & x*(y*z)&=(x*y)*z
\end{align*}
Finally, the following equations must be able to be solved:
\begin{gather*}
a + x = 0\\
a*x=1 \text{ assuming } a\neq 0
\end{gather*}

\paragraph{An example of a finite field}
A useful \textbf{finite field} is $GF(2)$. It has two elements, zero and one. Addition and multiplication is like usual, except that $1+1=0$.

\section{Analytical Geometry in the Plane}

\paragraph{Points:}
Given points P and Q with coordinates
$\begin{pmatrix}p_1 \\ p_2\end{pmatrix}$
and
$\begin{pmatrix}q_1 \\ q_2\end{pmatrix}$,
their distance $d$ can be computed using pythagoras:
$$d = \sqrt{(q_1-p_1)^2+(q_2-p_2)^2}$$

\paragraph{Vectors:}
\begin{itemize}
\item
Pair
$\begin{pmatrix} v_1 \\ v_2 \end{pmatrix}$
can be a movement of the plane: every point
$P=\begin{pmatrix}p_1 \\ p_2\end{pmatrix}$
is shifted to
$P'=\begin{pmatrix}p_1+v_1 \\ p_2+v_2\end{pmatrix}$
\item Uppercase letters are used for points and lowercase with arrows for vectors.
\item A vector has length
$|\vec{v}|=\sqrt{(v_1)^2 + (v_2)^2}$
which is the distance each point travels under the movement described by $\vec{v}$.
\item A Vector of length 1 is a Unit Vector.
\item $\vec{0}=\begin{pmatrix}0\\0\end{pmatrix}$ is the null vector.
\item $\vec{v}/|\vec{v}|$ is the unit vector pointing in the same direction as $\vec{v}$.
\item The vector $\vec{PQ}$ that moves point P into point Q has coordinates (q1-p1, q2-p2).
\item All points X that can be reached from P by following some distance along the direction of $\vec{v}$ lie on the straight line $X = P + (s \cdot v)$.
\item This is a parametric representation of a line where the parameter is s.
\item If given two points P and Q in the plane then this defines a straight line $X = P + (s \cdot PQ)$.
\item If given two lines $X = P + (s \cdot v)$ and $Y = Q + (t\cdot w)$ their point of intersection satisfies $p1 + sv1 = q1 + tw1$ and $p2 + sv2 = q2 + tw2$. This can be solved using Gaussian Elimination.
\end{itemize}

\section{Geometry in 3 Dimensions}

\paragraph{Planes}
The parametric representation of a plane has the form $X=P+s\cdot \vec{v}+t\cdot \vec{w}$ where P is a point in space and $\vec{v}$ and $\vec{w}$ are vectors (neither of which are null). $\vec{w}$ must not point in the same (or opposite) direction as $\vec{v}$, otherwise it is just a line.
\vspace{1mm}\\
Three points P, Q and R which are not all on the same line determine a plane $X=P+s\cdot \vec{PQ}+t\cdot \vec{PR}$.

\paragraph{Intersection Tasks}
\begin{itemize}
\item \textbf{Test whether a point lies on a line or plane} For point Q: $X = P + (s \cdot v)$ therefore $P + (s \cdot v) = Q$.
\item \textbf{Find the intersection point between lines/planes} Simply set the two equations as equal to each other.
\end{itemize}
Solve using Gaussian Elimination.

\paragraph{Two-point description of a line} Two points P and Q determine a line
$$X = P + s\cdot \vec{PQ}$$
This can be rewritten as
\begin{align*}
X &= P+s\cdot (Q-P)\\
&= P + s\cdot Q-s\cdot P\\
&= (1-s)\cdot P+s\cdot Q
\end{align*}
This can also be done with a plane using 3 points.

\paragraph{Vector spaces}
So we have a null vector, we can add two vectors, and we can multiply vectors with a scalar.
Any structure that satisfies the laws of vector algebra and carries these two operations is called a \textbf{vector space}.
\vspace{1mm}\\
The idea of a vector space is more complex than just 3-dimensional movements. Scalars can come from any field $\mathbb{F}$, for example $GF(2)$. One then speaks of a ``vector space over $\mathbb{F}$''.

\paragraph{Subspaces}
If $\vec{v}$ is an element of some vector space then we can generate a \textbf{subspace} (a \textbf{sub-vector space}) by considering all vectors of the form $s\cdot \vec{v}$. Another subspace would be all expressions of the form $s\cdot \vec{v} + t\cdot \vec{w}$.
\vspace{1mm}\\
Another way of looking at parametric representations is that we pick a point and allow all movements from a subspace to act on this point. This leads to an \textbf{affine subspace}. In other words, lines and planes are affine subspaces of 2D/3D.

\paragraph{Bases}
If two generators point in the same direction then one of them is redundant and can be removed. A set of generators that includes a redundant generator is said to be \textbf{linearly dependent}. A set of generators that can not be made any smaller is a \textbf{basis} of the subspace. The number of elements of a basis is the \textbf{dimension} of the subspace.
\vspace{1mm}\\
If we start with some set of generators and we are not sure if any of them are redundant, we can write the vectors as rows into a matrix and run Gaussian elimination. The rows of the resulting echelon form will be a basis for the subspace. Any redundancy will show itself as rows consisting entirely of zeros.

\paragraph{Codes}
Subspaces in $GF(2)^n$ are used as linear \textbf{codes} in coding theory. This is coding to spot and correct errors in data, not programming or cryptography.

\section{Vector Form}
\paragraph{A line from a linear equation}
The linear equation
$$x_1-2x_2=3$$
has the solutions
\begin{center}
\begin{tabular}{r c l}
$x_2$ & : & choose freely\\
$x_1$ & = & $3+2x_2$
\end{tabular}
\end{center}
This can be written in vector form as:
$$X=\begin{pmatrix}x_1 \\ x_2\end{pmatrix}
= \begin{pmatrix}3+2x_2 \\ x_2\end{pmatrix}
= \begin{pmatrix}3 \\ 0\end{pmatrix}
+ x_2\cdot \begin{pmatrix}2 \\ 1\end{pmatrix}$$
and we see that all solutions lie on a line.

\paragraph{Intersection tasks}
...become a lot easier when using vector form. For example, intersecting a line with a plane:
\begin{center}
\begin{tabular}{r l}
line: & $X=\begin{pmatrix}2\\1\\1\end{pmatrix}+s\cdot \begin{pmatrix}1\\0\\-3\end{pmatrix}$\\
plane: & $x_1-2x_2+3x_3=5$
\end{tabular}
\end{center}
To solve just substitute the line into the equation of the plane.
$$[2+s]-2[1]+3[1-3s]=5$$
It is much simpler to solve because a parametric representation \textit{generates points} whilst an equation \textit{tests points}.

\paragraph{Translating from parametric to equational}
\begin{description}
\item [Lines] If we are given the parametric representation of a line in 2D
$$X=P+s\cdot \vec{v}=\begin{pmatrix}p_1\\p_2\end{pmatrix}+s\cdot \begin{pmatrix}v_1\\v_2\end{pmatrix}$$
and we are looking for an equation
$$ax_1+bx_2=d$$
then we just need to set
\begin{center}
\begin{tabular}{r c l}
$a$ & $=$ & $-v_2$\\
$b$ & $=$ & $v_1$\\
$d$ & $=$ & $ap_1+bp_2$
\end{tabular}
\end{center}

\item [Planes] If we are given the parametric representation of a plane in 3D
$$X=P+s\cdot \vec{v}+t\cdot \vec{w}=\begin{pmatrix}p_1\\p_2\\p_3\end{pmatrix}+s\cdot \begin{pmatrix}v_1\\v_2\\v_3\end{pmatrix}+t\cdot \begin{pmatrix}w_1\\w_2\\w_3\end{pmatrix}$$
and we are looking for an equation
$$ax_1+bx_2+cx_3=d$$
then we just need to set
\begin{center}
\begin{tabular}{r c l}
$a$ & $=$ & $v_2w_3-v_3w_2$\\
$b$ & $=$ & $v_3w_1-v_1w_3$\\
$c$ & $=$ & $v_1w_2-v_2w_1$\\
$d$ & $=$ & $ap_1+bp_2+cp$
\end{tabular}
\end{center}
\end{description}

\section{The Inner Product}
\paragraph{Geometric interpretations}
Given a linear equation
$$ax_1+bx_2+cx_3=d$$
we assemble the coefficients in a vector
$$\vec{n}=\begin{pmatrix}a\\b\\c\end{pmatrix}$$
This vector is orthogonal to the plane. This is called the \textbf{normal} to the plane.
\vspace{1mm}\\
The right-hand side $d$ of the equation is negative if the origin is on the same side of the plane as the normal, and positive otherwise. The distance of the origin from the plane is given by
$$\text{distance}=\frac{d}{|\vec{n}|}$$
where $|\vec{n}|=\sqrt{a^2+b^2+c^2}$.
Since the normal can easily be computed from equations of the form $ax_1+bx_2+cx_3=d$, this is called the \textit{normal form}.

\paragraph{Inner Product}
The expression $ax_1+bx_2+cx_3$ can be viewed as the \textbf{inner product} of two tuples $\vec{n}=\begin{pmatrix}a\\b\\c\end{pmatrix}$ and $X=\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}$. This is denoted by $\langle\vec{n},X\rangle$. It is also known as the \textbf{dot product} ($\vec{n}\cdot X$).
\vspace{1mm}\\
The following laws apply:
\begin{align*}
  \langle\vec{v},\vec{w}\rangle&=\langle\vec{w},\vec{v}\rangle\\
  \langle\vec{v}+\vec{w},\vec{x}\rangle&=\langle\vec{v},\vec{x}\rangle+\langle\vec{w},\vec{x}\rangle\\
  \langle s\cdot\vec{v},\vec{w}\rangle&=s\times \langle\vec{v},\vec{w}\rangle
\end{align*}
If you know the normal to a plane and a point P on it, the normal form can be written as
$$\langle\vec{n},X\rangle=\langle\vec{n},P\rangle$$

\paragraph{Geometric properties}
\begin{description}
\item [Orthognality] Two vectors $\vec{v}$ and $\vec{w}$ are perpendicular if $\langle\vec{v},\vec{w}\rangle=0$.
\item[Angles] $\langle\vec{v},\vec{w}\rangle=|\vec{v}|\times|\vec{w}|\times cos(\alpha)$ where $\alpha$ is the angle between $\vec{v}$ and $\vec{w}$. $\langle\vec{v},\vec{w}\rangle$ is positive if the angle is acute and negative if the angle is obtuse.
\item [Length] $|\vec{v}|^2=\langle\vec{v},\vec{v}\rangle$
\item [Projection] We can compute the \textbf{orthogonal projection} of $\vec{v}$ onto the line defined by $\vec{w}$ by computing
$$\frac{\langle\vec{v},\vec{w}\rangle}{\langle\vec{w},\vec{w}\rangle}\cdot\vec{w}$$
which is the same as
$$\left\langle\vec{v},\frac{\vec{w}}{|\vec{w}|}\right\rangle\cdot\frac{\vec{w}}{|\vec{w}|}$$
\end{description}

\paragraph{Graphics-related tasks}
\begin{description}
\item [Computing the distance from a plane] If $E$ is a plane $\langle\vec{n},X\rangle =d$ and Q is a point then the distance $Q$ to $E$ (measured in the direction of the normal) is given by
$$\frac{d-\langle\vec{n},Q\rangle}{|\vec{n}|}=\frac{\langle\vec{n},P\rangle -\langle\vec{n},Q\rangle}{|\vec{n}|}$$
If this returns 0 then $Q$ is a point in the plane. If the result is negative then the normal is pointing towards the side that $Q$ is on.
\item [Nearest neighbour] To find which point on $E$ is closest to $Q$ we move from $Q$ to $E$ in the direction of the normal.
$$Q'=Q+\frac{d-\langle\vec{n},Q\rangle}{\langle\vec{n},\vec{n}\rangle}\cdot\vec{n}$$
If the plane goes through the origin then $d=0$ and the formula simplifies to
$Q'=Q-\frac{\langle\vec{n},Q\rangle}{\langle\vec{n},\vec{n}\rangle}\cdot\vec{n}$.
\item [Reflecting a point at a plane] Similar to the previous task, but we go twice the distance.
$$Q'=Q+2\times\frac{d-\langle\vec{n},Q\rangle}{\langle\vec{n},\vec{n}\rangle}\cdot\vec{n}$$
\item [Reflecting a line at a plane] Pick two different points on the line and reflect them according to the previous formula. Then plug them into the parametric formula given in section 4.
\end{description}

\paragraph{Higher dimensions} To represent a ``normal form'' for a line in 3D, we use two equations. The line of interest is the intersection of the two planes given by those equations. More generally, to represent an $m$-dimensional object in $d$-dimensional space, we can either use a parametric representation with $m$ non-redundant variables, or an equational representation with $d-m$ equations. Translating from one representation to the other is done through our old friend guassian elimination.

\paragraph{Normal forms and finite fields} We can represent subspaces in parametric form or equational form regardless of the underlying field.
\vspace{1mm}\\
One application of this is \textit{error-correcting codes}. We use a parametric representation to generate code words from data vectors, then use the equational representation to check whether a word recieved is valid. In this case the parametric form is called the \textit{generator matrix} and the equational form the \textit{check matrix}.
\vspace{1mm}\\
In order to use the notion of ``distance'', a field needs the following properties:
\begin{center}
\begin{tabular}{r c l}
$\langle\vec{v},\vec{v}\rangle$ & $\geq$ & 0\\
$\langle\vec{v},\vec{v}\rangle=0$ & $\implies$ & $\vec{v}=\vec{0}$
\end{tabular}
\end{center}
Notice that the second property does not hold on GF(2).
$$\left\langle\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\1\end{pmatrix}\right\rangle=1\times 1+1\times 1=1+1=0$$
Therefore there is no notion of distance in GF(2).

\section{Matrices}
\paragraph{Uses of matrices} A matrix is a rectangular arrangement of numbers. The \textbf{dimension} of a matrix gives the number of rows and columns. For example, the dimension of a matrix with 2 rows and 3 columns is $2\times 3$. (This is not an equation - the dimension is not 6).
\vspace{1mm}\\
Any number $\geq 1$ is allowed for the number of rows and columns in a matrix. A $1\times 1$ matrix is just a number, a $1\times 3$ matrix is just a vector, and a $3\times 1$ matrix is just a vector written as a column. A matrix is \textbf{square} if the number of rows equals the number of columns.
\vspace{1mm}\\
A \textbf{stochastic matrix} is a square matrix where the values in each column add up to 1. These can describe the behaviour of a \textbf{Markov chain}, a system that changes state probabilistically. Matrices can also describe linear transformations (see Graphics module).

\paragraph{Notation} Matrices are denoted by capital letters ($A$, $B$ etc.), while entries in a matrix are denoted $a_{ij}$, where $i$ is the row number and $j$ is the column number. Unlike most programming languages, indexing starts at 1.

\paragraph{Vector space operations} Matrices are compared \textit{componentwise}. This means that two matrices are equal only if they agree in their dimensions and in each position. Scalar multiplication (multiplying a matrix by a number) is done in the same way as on vectors.

\paragraph{Laws for matrix operations}
%TODO

\paragraph{Matrix multiplication} The result of multiplying an $m\times n$ matrix $A$ with an $n\times p$ matrix $B$ is an $m\times p$ matrix $C$ whose entry $c_{ik}$ in row $i$ and column $k$ is computed as
$$c_{ik}=\sum{n}{j=1}a_{ij}b_{jk}=a_{i1}\times b_{1k}+a_{i2}\times b_{2k}+...+a_{in}\times b_{nk}$$
In words, to compute the entry of matrix $C$ in row $i$ and column $k$, compute the inner product of the $i$th row of $A$ with the $k$th column of $B$. Clearly the width of the first matrix must equal the height of the second matrix if we want to multiply the two. It follows that multiplication of matrices is (usually) not commutative.
$$AB\neq BA$$

\paragraph{The Identity matrix} Every dimension $n\times n$ has its own version of the \textbf{identity matrix}. The $3\times 3$ matrix looks like this:
$$\begin{pmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{pmatrix}$$

\paragraph{The laws of matrix multiplication}
\begin{align*}
(AB)C&=A(BC)\\
A(B+C)&=AB+AC\\
(A+B)C&=AC+BC\\
AE&=EA=A\\
A0&=0A=0\\
s\cdot (AB)&=(s\cdot A)B=A(s\cdot B)
\end{align*}

\section{Invertibility of Matrices}
For every matrix $A$ there is almost always an inverse matrix $A^{-1}$. This matrix satisfies $A^{-1}A=E$ and $AA^{-1}=E$.
\vspace{1mm}\\
%TODO: Example
To compute the inverse of matrix $A$ we just use Guassian elimination to solve the equation $AA^{-1}=E$. Some matrices cannot be inverted (for example the zero matrix).
\vspace{1mm}\\
For $2\times 2$ matrices there is a simple formula for the inverse matrix.
$$\text{If }A=\begin{pmatrix}a&b\\c&d\end{pmatrix}\text{ then }A^{-1}=\frac{1}{ad-bc}\cdot\begin{pmatrix}d&-b\\-c&a\end{pmatrix}$$

\section{Sets}
A set is a collection of things. These ``things'' can be people, colours, or PDF lecture notes. They are known as \textbf{elements} or \textbf{members}. There is usually a shared attribute involved.

\paragraph{Rules of Sets}
\begin{itemize}
\item The elements of a set must be identifiable and distinguishable from each other. ``Dust in my room'' is not a set.
\item There must be a clear criteria that distinguishes ``things in the set'' from ``things not in the set''.
\item A member of a set is counted only once.
\item The elements of a set are not ordered.
\item A set is defined by which members it has and nothing else.
\end{itemize}

\paragraph{Notation} $x\in A$ indicates that $x$ is an element of $A$. Sets can also be written explicitly, e.g. \{1, 2, 3, 4, 5\} or \{1, 4, 9, 16, 25, ...\}.

\paragraph{New sets from old}
\begin{description}
\item [Pairing] The elements of the set $A\times B$ are pairs $(x,y)$ where $x$ is an element of $A$ and $y$ is an element of $B$. If both sets are equal ($A\times A$) we can call this $A^2$.
\item [Finite sequences] $A^*$ is the set whose elements are finite sequences of elements of $A$. For example, if set $A$ is an alphabet, then $A^*$ contains the words you can make with the elements of $A$. $\Sigma$ (capital sigma) is often used to denote alphabets. $A^*$ also contains the \textit{empty sequence}, often denoted by $\epsilon$ (epsilon).
\item [Subsets] If we have a set $A$ then we can create a set $B$ that consists of some elements from $A$. We can call this a \textbf{subset} of $A$. $B$ is a subset of $A$ only if all of the elements in $B$ can be found in $A$.
\vspace{2mm}\\
\textbf{Subset notation:}
$$B=\{x\in A\; |\; x\text{ satisfies condition } c\}$$
``$B$ is the set of all elements of $A$ for which condition $c$ is true.''
\vspace{2mm}\\
To indicate that one set is a subset of another, we use the symbol $\subseteq$.
$$B\subseteq A$$
Whatever the set $A$ is, we can always form two standard subsets:
\begin{gather*}
\{x\in A\; |\; true\}\\
\{x\in A\; |\; false\}
\end{gather*}
The first is equal to $A$, and the second is the \textbf{empty set} ($\emptyset$ or \{\}).
\vspace{2mm}\\
\textbf{Set definitions:}
\begin{gather*}
B\subseteq A\; \text{iff (if and only if)}\; \forall x.\: x\in B\implies x\in A\\
A=B\; \text{iff}\; A\subseteq B\; \text{and}\; B\subseteq A
\end{gather*}
\end{description}

\paragraph{Operations on subsets} Suppose $B$, $B_1$ and $B_2$ are subsets of a set $A$.
\begin{description}
\item [Union] The union of $B_1$ and $B_2$ is the combination of all elements from both sets. If an element is in both sets, it still only appears in the union once.
$$B_1 \cup B_2 = \{x\in A\; |\; x\in B_1\; \text{or}\; x\in B_2\}$$
\item [Intersection] The intersection of $B_1$ and $B_2$ is the set of elements that are in both $B_1$ and $B_2$.
$$B_1 \cap B_2 = \{x\in A\; |\; x\in B_1\; \text{and}\; x\in B_2\}$$
\item [Complement] The complement of $B$ is the set of elements that are in $A$ but \textit{not} in $B$.
$$\overline{B}=\{x\in A\; |\; x\notin B\}$$
\item [Difference] The difference of $B_1$ and $B_2$ is the set of elements that are in $B_1$ but not in $B_2$. (Note this is not the same as the \textit{compliment} since that compares a set to the alphabet)
$$B_1 \backslash B_2=B_1 \cup \overline{B_2}=\{ x\in A\; |\; x\in B_1\; \text{but}\; x\notin B_2$$
\end{description}

\paragraph{Power sets} The power set of $A$ ($\mathscr{P} A$) is the set of all sets that are subsets of $A$.

\section{Cardinalty: Countable and Uncountable sets}
\paragraph{Counting the elements of a set} The number of elements in a set is called the \textbf{Cardinality}. If set $A$ has four elements we can write this as
$$|A|=4\quad \text{or}\quad \text{card}(A)=4$$

\paragraph{Cardinality of infinite sets} Since the sets $\mathbb{N}$ (natural numbers), $\mathbb{Z}$ (integers), $\mathbb{Q}$ (rational numbers) and $\mathbb{R}$ (real numbers) are infinite, therefore it appears that their cardinality is ``infinity.'' However, this is not correct.
\vspace{1mm}\\
First consider the set of natural numbers $\mathbb{N}$. We call a set \textbf{countable} or \textbf{countably infinite} if it has the same cardinality as $\mathbb{N}$. We can prove that $\mathbb{Z}$ is countable by pairing each number in $\mathbb{Z}$ to a number in $\mathbb{N}$. For example:
$$(\mathbb{N,Z})=\{(0,0),(1,1),(2,-1),(3,2),(4,-2),...\}$$
We can also prove that $\mathbb{N}^2$ is countable:
$$(\mathbb{N,N}^2)=\{(0,(0,0)),(1,(1,0)),(2,(0,1)),(3,(2,0)),(4,(1,1)),(5,(0,2)),...\}$$
If $\Sigma$ is a finite alphabet, then the infinite set of words $\Sigma^*$ is countable (imagine walking breadth-first through a tree and pairing $\mathbb{N}$ with the values in the tree). The set of valid Java programs is therefore also countable.
\vspace{1mm}\\
We can prove that $\mathbb{R}$ is uncountable through \textbf{contradiction}. First assume that we have a listing of all the real numbers.
$$\begin{array}{l|c|ccccc}
  & c_0 & c_1 & c_2 & c_3 & c_4 & c_5\\
  \hline
  r_0 & 3. & 1 & 4 & 1 & 5 & 9...\\
  r_1 & 2. & 7 & 1 & 8 & 2 & 8...\\
  r_2 & 6. & 0 & 2 & 2 & 1 & 4...\\
  r_3 & 22. & 7 & 1 & 0 & 9 & 8...\\
  r_4 & 9. & 8 & 0 & 6 & 6 & 5...
\end{array}$$
Now consider a real number $a$. The digits of this number are labelled $a_0.a_1a_2a_3...$, where $a_0$ is the digit to the left of the decimal point. The value of the digit $a_i$ depends on the table listed above:
\begin{gather*}
  \text{If}\ r_ic_i\neq 0 \implies a_i=0\\
  \text{If}\ r_ic_i=0 \implies a_i=1
\end{gather*}
In other words, we go diagonally down through the table and create $a$ according to the digits we find. This means that the number $a$ differs from \textit{every} number in the table of real numbers (it differs from the first number in the digit to the left of the decimal point, it differs from the second number in the digit to the right of the decimal point etc). Since $a$ is differs from every number in the table, it is certainly not represented in the table. Therefore, any listing of real numbers \textit{has} to miss out some real numbers. We can now say that the set of real numbers is uncountable. It also follows that, since the set of valid Java programs is countable, there must exist non-computable real numbers.

\paragraph{Powersets again} We can adapt the proof for real numbers to show that the powerset of $\mathbb{N}$ is uncountable. First, we encode each subset of $\mathbb{N}$ by a real number between 0 and 1, written in binary. For example, the set $A=\{1,2,4\}$ corresponds to $0.1101000...$. This has a one-to-one connection with the set of real numbers between 0 and 1, which is uncountable, therefore $\mathscr{P}\mathbb{N}$ must also be uncountable.
\vspace{1mm}\\
A powerset always has more elements than the set itself. Therefore, the sets in the sequence
$$\mathbb{N}\quad \mathscr{P}\mathbb{N}\quad \mathscr{PP}\mathbb{N}\quad \mathscr{PPP}\mathbb{N}...$$
have greater and greater cardinality. It's infinitely infinite!

\section{Relations}
\paragraph{General relations} If $A_1,A_2,...,A_n$ are sets and $R$ is a subset of $A_1\times a_2\times ...\times A_n$, then $R$ is a ($n$-ary) \textbf{relation}.

\end{document}
